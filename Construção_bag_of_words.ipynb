{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Sua tarefa será  gerar a matriz termo documento, dos documentos recuperados da internet e \n",
        "imprimir esta matriz na tela. Para tanto: \n",
        "a) Considere que todas as listas de sentenças devem ser transformadas em listas de vetores, \n",
        "onde cada item será uma das palavras da sentença. \n",
        "b) Todos  os  vetores  devem  ser  unidos  em  um  corpus  único  formando  uma  lista  de  vetores, \n",
        "onde cada item será um lexema.  \n",
        "c) Este único corpus será usado para gerar o vocabulário. \n",
        "d) O  resultado  esperado  será  uma  matriz  termo  documento  criada  a  partir  da  aplicação  da \n",
        "técnica bag of Words em todo o corpus.  \n",
        " "
      ],
      "metadata": {
        "id": "OdZN3Pk4wXhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Aluno: Fernando Purkott\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "\n",
        "@Language.component(\"Space\")\n",
        "def set_custom_boundaries(doc):\n",
        "     for token in doc[:-1]:\n",
        "         if token.text.__contains__('  '):\n",
        "             doc[token.i+1].is_sent_start = True\n",
        "     return doc\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.add_pipe('Space', before=\"parser\")\n",
        "\n",
        "def excluiNovaLinha(value):\n",
        "    return ''.join(value.splitlines())\n",
        "\n",
        "\n",
        "\n",
        "blacklist = [\n",
        "    '[document]',\n",
        "    'noscript',\n",
        "    'header',\n",
        "    'html',\n",
        "    'meta',\n",
        "    'head', \n",
        "    'input',\n",
        "    'script',\n",
        "    'style'\n",
        "]\n",
        "\n",
        "def scrape(URL):\n",
        "  blacklist = [\n",
        "    '[document]',\n",
        "    'noscript',\n",
        "    'header',\n",
        "    'html',\n",
        "    'meta',\n",
        "    'head', \n",
        "    'input',\n",
        "    'script',\n",
        "    'style'\n",
        "  ]\n",
        "\n",
        "  req = requests.get(URL)\n",
        "  soup = BeautifulSoup(req.content)\n",
        "\n",
        "\n",
        "  txt = soup.find_all(text=True)\n",
        "  clean_txt = \"\"\n",
        "\n",
        "  for t in txt:\n",
        "    if t.parent.name not in blacklist:\n",
        "        clean_txt += '{} '.format(t)\n",
        "\n",
        "  txt_doc = nlp(excluiNovaLinha(clean_txt))\n",
        "\n",
        "\n",
        "  sentences = list(txt_doc.sents)\n",
        "  return sentences\n",
        "\n",
        "def criaVocab(sentences):\n",
        "  vocab = []\n",
        "  sentenceWords = []\n",
        "  for s in sentences:\n",
        "    for i in s:\n",
        "      sentenceWords = str(i).split()\n",
        "      for j in sentenceWords:\n",
        "        j = j.lower()\n",
        "        if j not in stop_words and j != '':\n",
        "          vocab.append(j.translate(str.maketrans('', '', string.punctuation)))  \n",
        "      sentenceWords = []\n",
        "    vocab = unico(vocab)\n",
        "    return vocab\n",
        "  \n",
        "\n",
        "def unico(words):\n",
        "  seen = set()\n",
        "  return [x for x in words if not (x in seen or seen.add(x))]\n",
        "\n",
        "def vectorize(tokens, vocab):\n",
        "    vector=[]\n",
        "    for w in vocab:\n",
        "        vector.append(tokens.count(w))\n",
        "    return vector\n",
        "\n",
        "\n",
        "# \"https://www.ibm.com/cloud/learn/natural-language-processing\"\n",
        "\n",
        "sentences1 = scrape(\"https://www.ibm.com/cloud/learn/natural-language-processing\")\n",
        "sentences2 = scrape(\"https://hbr.org/2022/04/the-power-of-natural-language-processing\")\n",
        "sentences3 = scrape(\"https://link.springer.com/article/10.1007/s11042-022-13428-4\")\n",
        "sentences4 = scrape(\"https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP\")\n",
        "sentences5 = scrape(\"https://monkeylearn.com/natural-language-processing/\")\n",
        "\n",
        "sentences = [sentences1, sentences2, sentences3, sentences4, sentences5]\n",
        "\n",
        "vocab = criaVocab(sentences)\n",
        "\n",
        "print(vocab)\n",
        "for i in sentences:\n",
        "  for j in i:\n",
        "    tokens = str(j).split()\n",
        "    vector = vectorize(tokens, vocab)\n",
        "    print(vector)\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "nIbYFUhUMiG8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1de0d6f-5944-4259-bbb8-9da384d3c975"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}